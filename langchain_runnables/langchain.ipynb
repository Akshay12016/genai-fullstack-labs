{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e2bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab47fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO LANGCHAIN'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "upper_runnable = RunnableLambda(to_upper)\n",
    "\n",
    "upper_runnable.invoke(\"hello langchain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a3f059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import certifi\n",
    "print(os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89dabd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08af133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apoliset\\AppData\\Local\\Temp\\ipykernel_28140\\3768421699.py:5: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A very specific and technical question!\\n\\nIn the context of LangChain, a tool for building AI models, \"Runnables\" refer to small, reusable pieces of code that can be executed on top of a large language model (LLaMA) or other AI models.\\n\\nThink of Runnables like Lego blocks. Just as you can combine different Lego blocks to build various structures, LangChain Runnables are pre-built blocks of code that can be connected together in different ways to perform specific tasks or functions.\\n\\nRunnables can do things like:\\n\\n1. **Text processing**: Runnables can perform tasks such as tokenization (breaking text into individual words), entity recognition (identifying important entities like names, locations), or sentiment analysis (determining the emotional tone of text).\\n2. **AI model integration**: Runnables can interface with other AI models, allowing you to leverage their capabilities in your own projects.\\n3. **Data manipulation**: Runnables can perform various data-related tasks, such as filtering, sorting, or aggregating data.\\n\\nBy combining different Runnables, you can create more complex workflows and automate various tasks without having to write custom code from scratch. This makes it easier to develop AI-powered applications and integrate them with other tools and systems.\\n\\nIn summary, LangChain Runnables are reusable, modular pieces of code that enable you to build upon existing AI models and perform specific tasks or functions in your projects.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Explain {question} in simple terms\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"question\": \"What are LangChain Runnables?\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
